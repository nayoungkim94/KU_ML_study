{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 09-2 : XOR prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() - default in tf  2.0\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시 데이터 \n",
    "x: 2차원 array\n",
    "-> x1,x2 기준으로 y를 0 or 1로 구분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "[0, 1],\n",
    "[1, 0],\n",
    "[1, 1]]\n",
    "y_data = [[0],\n",
    "[1],\n",
    "[1],\n",
    "[0]]\n",
    "\n",
    "plt.scatter(x_data[0][0],x_data[0][1],c='red',marker='.')\n",
    "\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "##if 함수 써서 표현?\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).shuffle(50000).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##weight&bias 생성 \n",
    "#random_normal 2.0에서 사용불가\n",
    "W1 = tf.Variable(tf.random.normal([2, 1]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([1]), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1]), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([2, 1]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([1]), name='bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 생성\n",
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)  ## .nn 없이 사용가능 ##softmax는 사용불가 nn. or math.필요\n",
    "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "    layer3 = tf.concat([layer1, layer2],-1) ## axis=-1\n",
    "    layer3 = tf.reshape(layer3, shape = [-1,2])\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
    "    return hypothesis\n",
    "#loss func(in logistic reg) \n",
    "#.math.log로 수정할 것\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) ##boolean 값을 float32값으로 cast\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32)) #실제 y값과 예측값의 비교\n",
    "    return accuracy\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])\n",
    "#keras - loss func & accuracy 이용\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.7016\n",
      "Iter: 5000, Loss: 0.6894\n",
      "Iter: 10000, Loss: 0.6732\n",
      "Iter: 15000, Loss: 0.6322\n",
      "Iter: 20000, Loss: 0.5765\n",
      "Iter: 25000, Loss: 0.5298\n",
      "Iter: 30000, Loss: 0.4297\n",
      "Iter: 35000, Loss: 0.2767\n",
      "Iter: 40000, Loss: 0.1617\n",
      "Iter: 45000, Loss: 0.1036\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50000\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(neural_net(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3]))\n",
    "        if step % 5000 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(neural_net(features),labels)))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))\n",
    "##시간 단축 방법???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 09 -4 tensorboard \n",
    "\n",
    "## Solve XOR prob with 4-layer NN \n",
    "\n",
    "* layer 1 : 2,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([10]), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([10]), name='bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([10]), name='bias3')\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random.normal([1]), name='bias4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eager default 이므로 contrib 모두 제거\n",
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "    with tf.summary.record_summaries_every_n_global_steps(1):\n",
    "        tf.summary.histogram(\"weights1\", W1)\n",
    "        tf.summary.histogram(\"biases1\", b1)\n",
    "        tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "        tf.summary.histogram(\"weights2\", W2)\n",
    "        tf.summary.histogram(\"biases2\", b2)\n",
    "        tf.summary.histogram(\"layer2\", layer2)\n",
    "\n",
    "        tf.summary.histogram(\"weights3\", W3)\n",
    "        tf.summary.histogram(\"biases3\", b3)\n",
    "        tf.summary.histogram(\"layer3\", layer3)\n",
    "\n",
    "        tf.summary.histogram(\"weights4\", W4)\n",
    "        tf.summary.histogram(\"biases4\", b4)\n",
    "        tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.log(hypothesis) + (1 - labels) * tf.log(1 - hypothesis))\n",
    "    with tf.summary.record_summaries_every_n_global_steps(1):\n",
    "        tf.summary.scalar('loss', cost)\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, W4, b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-e1f81b9aaa71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mglobal_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[0;32m    298\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "EPOCHS = 50000\n",
    "log_path = '/tmp/summaries/train'\n",
    "writer = tf.summary.create_file_writer(log_path)\n",
    "##수정:writer = tf.contrib.summary.create_file_writer(log_path)에서 위와 같이 변경\n",
    "global_step=tf.compat.v1.train.get_or_create_global_step()     \n",
    "##수정:global_step=tf.train.get_or_create_global_step()  \n",
    "# global step variable\n",
    "writer.set_as_default()\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    global_step.assign_add(1)\n",
    "    for features, labels  in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(neural_net(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, W4, b1, b2, b3, b4]))\n",
    "        if step % 50 == 0:\n",
    "            loss_value = loss_fn(neural_net(features),labels)\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
